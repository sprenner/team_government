{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, hamming_loss, confusion_matrix\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "Load your training data and pass column names for the columns you have saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv( ... ,sep=',', header=None, names=[ ... , ... ], index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop words\n",
    "\n",
    "Use stop_words to remove less-meaningful words. The logic of removing stop words has to do with the fact that these words don't carry a lot of meaning, and they appear a lot in most text. Read the list of stopwords, strip and decode them like in the first exercise.\n",
    "\n",
    "Hint: retweets are marked in the text, you might add this marker to your stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import unidecode\n",
    "\n",
    "with io.open( ... , mode= ... , encoding= ... ) as f:\n",
    "      content = f.readlines()\n",
    "content = [... for x in content]\n",
    "content = [... for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ML pipeline\n",
    "\n",
    "Define a ML pipeline below by setting the respective parameters to values of your choice.\n",
    "\n",
    "The [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) combines the functionalities of a CountVectorizer and a TfidfTransformer (read [this short explanation](http://www.tfidf.com/) to grasp the fundamental idea of tfidf). The parameters you could play around with are:\n",
    "\n",
    "**`ngram_range`:** Set `ngram_range` to `(1,1)` for outputting only one-word tokens, `(1,2)` for one-word and two-word tokens, `(2, 3)` for two-word and three-word tokens, etc.\n",
    "\n",
    "**`analyzer`:** `ngram_range` works hand-in-hand with analyzer. Set analyzer to \"word\" for outputting words and phrases, or set it to \"char\" to output character ngrams.\n",
    "\n",
    "**Advanced:** If you want your output to have both \"word\" and \"char\" features, use sklearn's `FeatureUnion`.\n",
    "\n",
    "\n",
    "**`max_df`:** Ignore words with a document frequency higher than this value (float between `0.0` and `1.0`).\n",
    "Since stop words generally have a high frequency, it might make sense to use `max_df` as a float of say 0.95 to remove the top 5% but then you're assuming that the top 5% is all stop words which might not be the case. It really depends on your text data. In some lines of work, it's very common that the top words or phrases are NOT stop words because you work with dense text (search query data) in very specific topics.\n",
    "\n",
    "**`min_df`:** Ignore words with a document frequency lower than this value (float between `0.0` and `1.0`). Use `min_df` as an integer to remove rare-occurring words. If they only occur once or twice, they won't add much value and are usually really obscure. Furthermore, there's generally a lot of them so ignoring them with say `min_df=5` can greatly reduce your memory consumption and data size.\n",
    "\n",
    "**Advanced:** `token_pattern` allows you to use a regex pattern e.g. `\\b\\w\\w+\\b` which means that tokens have to be at least 2 characters long so words like \"I\", \"a\" are removed and also numbers like 0 - 9 are removed. You'll also notice it removes apostrophes. It is only used if `analyzer == 'word'`.\n",
    "\n",
    "As a classifier we will use a linear support vector machine. This algorithm has proven to be very effective in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\n",
    "        'tfidv',\n",
    "        TfidfVectorizer(\n",
    "            ngram_range=(... , ...), \n",
    "            analyzer= ..., \n",
    "            strip_accents = 'unicode', \n",
    "            use_idf = True, #NOTE: use_idf=False AND norm=None is equivalent to using sklearn's CountVectorizer. It will just return counts.\n",
    "            stop_words= ... ,\n",
    "            sublinear_tf=True, \n",
    "            max_features=100, # if not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "            min_df= ..., \n",
    "            max_df= ...\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        'lin_svc',\n",
    "        svm.SVC(\n",
    "            C=1.0,\n",
    "            probability=True,\n",
    "            kernel='linear'\n",
    "        )\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(true, pred):\n",
    "    print('Accuracy:', accuracy_score(true, pred))\n",
    "    print('F1:', f1_score(true, pred, average='weighted'))\n",
    "    print('Precision:', precision_score(true, pred, average='weighted'))\n",
    "    print('Hamming loss', hamming_loss(true, pred))\n",
    "\n",
    "\n",
    "score_model(y_test,pipeline.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "\n",
    "Save your final model as `YOURTEAM_model.pkl` using joblib's `dump()` function, with `compress=3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(..., ..., ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
